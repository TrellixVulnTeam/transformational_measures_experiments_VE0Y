
* Check wtf is wrong with NM + mnist when not using any conv agg => values are too high!

* Define more experiment configurations
    * Invariance vs epochs trained, Â¿for all datasets/models? (how to implement?)DONE

    * Stratified vs none -> mnist/cifar, simple_conv/resnet DONE

    * Invariance to rotation: train with n rotations, test with more. DONE

    * Transformation strength vs invariance obtained DONE

    * Invariance to X vs epochs needed to train

    * Which model is more invariant

    * Train invariance to X in cifar with 5 classes, then test with other 5 classes. Is the invariance still there?

    * Train without invariance, then train with invariance to X, then test invariance to Y in both models.
    Does invariance to one thing helps in invariance to another?

    * Retraining: get a previously trained model and retrain with another set of transformations. Use only one measure (NM/Anova)
        * Vanilla => Affine
        * Affine => Vanilla
        * Rotation => Other Affine (and viceversa)
        * Rotation => More rotation angles
        * Rotation => Less rotation angles
     * Invariance vs number of layers

* Convert cmd arguments from fixed set of choices to separate options. IMPORTANT!!!!
    * Make each parameters object implement a get_parser() to get the ArgParser for the parameter, so that they can be reused in different scripts.
    * They should also implement a to_cmd() method, that generates the command line string representation, so that the runners can create Parameter objects and then just use to_cmd() to call the other scripts
    * Also centralize somewhere the map from a parameter object to a filename which contains the result.

* Retraining experiments; which layers get the invariance now?
